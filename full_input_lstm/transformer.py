"""
NOTE: Starter code generated by gpt-o4
"""
import torch
import torch.nn as nn
import math

from constants import FUTURE_STEPS

class PositionalEncoding(nn.Module):
    """Sinusoidal positional encoding, as in Vaswani et al."""
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                             -(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # shape (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x: (batch, seq_len, d_model)
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len]  # broadcast over batch


class TwoStageTransformerPredictor(nn.Module):
    """
    Two-stage Transformer: first cross-agent attention per timestep,
    then cross-time attention per agent.
    """
    def __init__(self,
                 num_features: int,
                 d_model: int = 64,  # orig 64
                 nhead: int = 4,
                 num_layers_spatial: int = 1,
                 num_layers_temporal: int = 1,
                 dim_feedforward: int = 128,
                 dropout: float = 0.1,
                 future_steps: int = 60,
                 output_dim: int = None):
        super().__init__()
        self.output_dim   = output_dim or num_features
        self.future_steps = future_steps

        # ─── embedding tables ─────────────────────────────────────────────
        num_agents = 50
        seq_len = 50
#        # one vector per agent
#        self.agent_embed = nn.Embedding(num_agents, d_model)
#        # one vector per input timestep
#        self.time_embed  = nn.Embedding(seq_len,  d_model)

        # Input projection + norm
        self.input_proj = nn.Linear(num_features, d_model)
        self.input_norm = nn.LayerNorm(d_model)

        # 1) Spatial encoder: cross-agent attention
        spatial_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation='gelu'
        )
        self.spatial_encoder = nn.TransformerEncoder(
            spatial_layer,
            num_layers_spatial
        )
        self.spatial_norm = nn.LayerNorm(d_model)

        # 2) Positional encoding for time axis
        self.time_pos_encoder = PositionalEncoding(d_model)

        # 3) Temporal encoder: cross-time attention
        temporal_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation='gelu'
        )
        self.temporal_encoder = nn.TransformerEncoder(
            temporal_layer,
            num_layers_temporal
        )
        self.temporal_norm = nn.LayerNorm(d_model)

        # Final norm + output heads
        self.final_norm = nn.LayerNorm(d_model)
        self.reconstruct = nn.Linear(d_model, self.output_dim)
        self.forecast    = nn.Linear(d_model, self.future_steps * self.output_dim)
#        hidden_dim = 64
#        self.forecast = nn.Sequential(
#            nn.Linear(d_model, hidden_dim),
#            nn.GELU(),
#            nn.Linear(hidden_dim, self.future_steps * self.output_dim)
#        )

    def forward(self, input_data):
        # Unpack DataBatch or use raw tensor
        if hasattr(input_data, 'x') and hasattr(input_data, 'num_graphs'):
            B = input_data.num_graphs
            # assuming x shape is (B*A, T, F)
            # reshape to (B, A, T, F)
            n_agents, seq_len, feat = 50, 50, input_data.x.size(-1)
            x = input_data.x.view(B, n_agents, seq_len, feat)
        else:
            x = input_data  # (B, A, T, F)

        # Keep last N timesteps (optional)
        x = x[:, :, -25:, :]
        B, A, T, F = x.shape

        # Project to model dim
        h = self.input_proj(x)

        # 2) add agent & time embeddings
        #    agent_ids: (A,)         → embeds → (A, d_model)
        #    time_ids:  (T,)         → embeds → (T, d_model)
        #    then reshape+broadcast to (B, A, T, d_model)
#        agent_ids = torch.arange(A, device=h.device)
#        time_ids  = torch.arange(T, device=h.device)
#        agent_emb = self.agent_embed(agent_ids).unsqueeze(0).unsqueeze(2)  # (1, A, 1, d)
#        time_emb  = self.time_embed(time_ids).unsqueeze(0).unsqueeze(1)  # (1, 1, T, d)
#        h = h + agent_emb + time_emb

        h = self.input_norm(h)            # → (B, A, T, d_model)

        # --- 1) Spatial: cross-agent attention per timestep ---
        # reshape to (B*T, A, d_model) then (A, B*T, d_model)
        h_sp = h.view(B * T, A, -1).transpose(0, 1)
        h_sp = self.spatial_encoder(h_sp)
        # back to (B, A, T, d_model)
        h_sp = h_sp.transpose(0, 1).view(B, T, A, -1).permute(0, 2, 1, 3)
        h_sp = self.spatial_norm(h_sp)

        # --- 2) Temporal: cross-time attention per agent ---
        # reshape to (B*A, T, d_model)
        h_tm = h_sp.permute(0, 2, 1, 3).reshape(B * A, T, -1)
        # add time positional encoding
        h_tm = self.time_pos_encoder(h_tm)
        # to (T, B*A, d_model) for Transformer
        h_tm = h_tm.transpose(0, 1)
        h_tm = self.temporal_encoder(h_tm)
        # back to (B, A, T, d_model)
        h_tm = h_tm.transpose(0, 1).reshape(B, A, T, -1)
        h_tm = self.temporal_norm(h_tm)

        # Final norm + heads
        h_final = self.final_norm(h_tm)
        recon = self.reconstruct(h_final)
        last  = h_final[:, :, -1, :]
        fut   = self.forecast(last).view(B, A, self.future_steps, self.output_dim)

        return fut


class AutoRegressiveMLP(nn.Module):
    def __init__(self,
                 num_features: int,
                 hidden_dim: int = 512,
                 output_dim: int = None,
                 future_steps: int = FUTURE_STEPS):
        """
        :param num_features: number of input features per timestep (F)
        :param hidden_dim:   dimensionality of the MLP’s hidden layers
        :param output_dim:   per‐feature output dim (defaults to num_features)
        :param future_steps: how many steps ahead to forecast (kept simple here)
        """
        super().__init__()
        self.output_dim   = output_dim or num_features
        self.future_steps = future_steps

        # 1) project raw features → hidden_dim
        self.input_proj = nn.Linear(num_features, hidden_dim)

        # 2) a tiny MLP applied per “agent×time token”
        self.mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
        )

        # 3) head to reconstruct each input step (optional)
        self.reconstruct = nn.Linear(hidden_dim, self.output_dim)

        # 4) head to forecast future_steps
        self.forecast = nn.Linear(hidden_dim, future_steps * self.output_dim)

    def forward(self, input_data):
        """
        x: Tensor of shape (B, A, T, F), or a DataBatch with .x & .num_graphs
        returns:
          • recon: (B, A, T, output_dim)
          • fut  : (B, A, future_steps, output_dim)
        """
        # --- unpack DataBatch if needed ---
        if hasattr(input_data, "x") and hasattr(input_data, "num_graphs"):
            # assume every graph has A=50 agents, T=50 timesteps, F=6 features
            B = input_data.num_graphs
            A, T, F = 50, 50, 6
            x = input_data.x.view(B, A, T, F)
        else:
            x = input_data  # already (B, A, T, F)

        # only keep the last 10 steps (as in your example)
        x = x[:, :, -10:, :]       # → (B, A, T′=10, F)
        B, A, T, F = x.shape

        # 1) project & flatten agent×time into tokens
        x = self.input_proj(x)     # → (B, A, T, hidden_dim)
        x = x.view(B, A * T, -1)   # → (B, A*T, hidden_dim)

        # 2) per‐token MLP
        h = self.mlp(x)            # → (B, A*T, hidden_dim)

        # 3) reshape back to (B, A, T, hidden_dim)
        h = h.view(B, A, T, -1)

        # 4) reconstruct
        recon = self.reconstruct(h)   # → (B, A, T, output_dim)

        # 5) forecast from the last time‐step
        last = h[:, :, -1, :]                                     # (B, A, hidden_dim)
        fut  = self.forecast(last)                                # (B, A, future_steps*output_dim)
        fut  = fut.view(B, A, self.future_steps, self.output_dim) # (B, A, future_steps, output_dim)

        return fut.squeeze(2)



