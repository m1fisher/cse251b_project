"""
NOTE: Starter code generated by gpt-o4
"""
import torch
import torch.nn as nn
import math

from constants import FUTURE_STEPS

class PositionalEncoding(nn.Module):
    """Sinusoidal positional encoding, as in Vaswani et al."""
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                             -(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # shape (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x: (batch, seq_len, d_model)
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len]  # broadcast over batch


class CrossAgentTransformerPredictor(nn.Module):
    def __init__(self,
                 num_features: int,
                 d_model: int = 24,
                 nhead: int = 4,
                 num_layers: int = 2,
                 dim_feedforward: int = 128,
                 dropout: float = 0.1,
                 future_steps: int = FUTURE_STEPS,
                 output_dim: int = None):
        super().__init__()
        self.output_dim = output_dim or num_features
        self.future_steps = future_steps

        # 1) Project raw features → model dimension
        self.input_proj = nn.Linear(num_features, d_model)
        self.input_norm = nn.LayerNorm(d_model)
        # 2) Positional encoding over (agent × time) tokens
        self.pos_encoder = PositionalEncoding(d_model)
        # 3) Transformer encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation='gelu'
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        self.final_norm = nn.LayerNorm(d_model)
        self.reconstruct = nn.Linear(d_model, self.output_dim)
        self.forecast = nn.Linear(d_model, self.future_steps * self.output_dim)

    def forward(self, input_data):
        """
        x: (B, A, T, F)
        returns:
          • recon: (B, A, T, output_dim)       — reconstruct input
          • fut  : (B, A, future_steps, output_dim) — forecast
        """
        if hasattr(input_data, "x") and hasattr(input_data, "num_graphs"):
            n_agents, seq_len, d_in = 50, 50, 6
            x = input_data.x.view(input_data.num_graphs, n_agents, seq_len, d_in)
        else:
            x = input_data
        # Only consider one second of time
        # TODO: try more time
        # TODO: visualize high MSE scenes
        x = x[:, :, -10:, :]
        B, A, T, F = x.shape

        # 1) merge agent×time
        x = self.input_proj(x)            # → (B, A, T, d_model)
        x = self.input_norm(x)
        x = x.view(B, A*T, -1)            # → (B, A*T, d_model)
        x = self.pos_encoder(x)           # → (B, A*T, d_model)
        x = x.transpose(0, 1)             # → (A*T, B, d_model)
        h = self.transformer(x)               # → (A*T, B, d_model)
        h = h.transpose(0, 1)             # → (B, A*T, d_model)
        h = h.view(B, A, T, -1)           # → (B, A, T, d_model)
        h = self.final_norm(h)

        # 2) reconstruction at each step
        recon = self.reconstruct(h)       # → (B, A, T, output_dim)

        # 3) forecasting from last time-step
        last = h[:, :, -1, :]             # → (B, A, d_model)
        fut  = self.forecast(last)        # → (B, A, future_steps*output_dim)
        fut  = fut.view(B, A, self.future_steps, self.output_dim)
        return fut


class AgentOnlyTransformerPredictor(nn.Module):
    def __init__(self,
                 num_features: int,
                 d_model: int = 24,
                 nhead: int = 4,
                 num_layers: int = 2,
                 dim_feedforward: int = 128,
                 dropout: float = 0.1,
                 future_steps: int = FUTURE_STEPS,
                 output_dim: int = None):
        super().__init__()
        self.output_dim = output_dim or num_features
        self.future_steps = future_steps

        self.input_proj = nn.Linear(num_features, d_model)
        self.input_norm = nn.LayerNorm(d_model)
        # this transformer now does *only* cross-agent attention
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation='gelu'
        )
        self.cross_agent_encoder = nn.TransformerEncoder(
            encoder_layer, num_layers
        )
        self.final_norm = nn.LayerNorm(d_model)
        self.reconstruct = nn.Linear(d_model, self.output_dim)
        self.forecast    = nn.Linear(d_model, self.future_steps * self.output_dim)

    def forward(self, input_data):
        # 1) unpack DataBatch or assume tensor
        if hasattr(input_data, "x") and hasattr(input_data, "num_graphs"):
            B = input_data.num_graphs
            x = input_data.x.view(B, 50, 50, 6)
        else:
            x = input_data
        # 2) take last 10 timesteps
        x = x[:, :, -10:, :]           # → (B, A, T, F) with T=10

        B, A, T, F = x.shape
        # 3) project into d_model
        h = self.input_proj(x)         # → (B, A, T, d_model)
        h = self.input_norm(h)

        # 4) reshape for cross-agent attention:
        #    (B, A, T, d) → (T, B, A, d) → (A, B*T, d)
        h = h.permute(2, 0, 1, 3)                          # (T, B, A, d)
        h = h.reshape(T * B, A, -1)                       # (B*T, A, d)
        h = h.transpose(0, 1)                             # (A, B*T, d)

        # 5) run *only* cross-agent encoder (attends across A)
        h = self.cross_agent_encoder(h)                   # (A, B*T, d)

        # 6) undo transforms → back to (B, A, T, d)
        h = h.transpose(0, 1).reshape(T, B, A, -1)        # (T, B, A, d)
        h = h.permute(1, 2, 0, 3)                         # (B, A, T, d)

        # 7) final norm & heads
        h = self.final_norm(h)                            # (B, A, T, d)
        recon = self.reconstruct(h)                       # (B, A, T, out_dim)

        last = h[:, :, -1, :]                             # (B, A, d)
        fut  = self.forecast(last)                        # (B, A, future_steps*out_dim)
        fut  = fut.view(B, A, self.future_steps, self.output_dim)

        return fut

class TwoStageTransformerPredictor(nn.Module):
    """
    Two-stage Transformer: first cross-agent attention per timestep,
    then cross-time attention per agent.
    """
    def __init__(self,
                 num_features: int,
                 d_model: int = 64,
                 nhead: int = 4,
                 num_layers_spatial: int = 1,
                 num_layers_temporal: int = 1,
                 dim_feedforward: int = 128,
                 dropout: float = 0.1,
                 future_steps: int = 60,
                 output_dim: int = None):
        super().__init__()
        self.output_dim   = output_dim or num_features
        self.future_steps = future_steps

        # Input projection + norm
        self.input_proj = nn.Linear(num_features, d_model)
        self.input_norm = nn.LayerNorm(d_model)

        # 1) Spatial encoder: cross-agent attention
        spatial_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation='gelu'
        )
        self.spatial_encoder = nn.TransformerEncoder(
            spatial_layer,
            num_layers_spatial
        )
        self.spatial_norm = nn.LayerNorm(d_model)

        # 2) Positional encoding for time axis
        self.time_pos_encoder = PositionalEncoding(d_model)

        # 3) Temporal encoder: cross-time attention
        temporal_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation='gelu'
        )
        self.temporal_encoder = nn.TransformerEncoder(
            temporal_layer,
            num_layers_temporal
        )
        self.temporal_norm = nn.LayerNorm(d_model)

        # Final norm + output heads
        self.final_norm = nn.LayerNorm(d_model)
        self.reconstruct = nn.Linear(d_model, self.output_dim)
        self.forecast    = nn.Linear(d_model, self.future_steps * self.output_dim)

    def forward(self, input_data):
        # Unpack DataBatch or use raw tensor
        if hasattr(input_data, 'x') and hasattr(input_data, 'num_graphs'):
            B = input_data.num_graphs
            # assuming x shape is (B*A, T, F)
            # reshape to (B, A, T, F)
            n_agents, seq_len, feat = 50, 50, input_data.x.size(-1)
            x = input_data.x.view(B, n_agents, seq_len, feat)
        else:
            x = input_data  # (B, A, T, F)

        # Keep last timesteps (optional)
        x = x[:, :, -25:, :]
        B, A, T, F = x.shape

        # Project to model dim
        h = self.input_proj(x)
        h = self.input_norm(h)            # → (B, A, T, d_model)

        # --- 1) Spatial: cross-agent attention per timestep ---
        # reshape to (B*T, A, d_model) then (A, B*T, d_model)
        h_sp = h.view(B * T, A, -1).transpose(0, 1)
        h_sp = self.spatial_encoder(h_sp)
        # back to (B, A, T, d_model)
        h_sp = h_sp.transpose(0, 1).view(B, T, A, -1).permute(0, 2, 1, 3)
        h_sp = self.spatial_norm(h_sp)

        # --- 2) Temporal: cross-time attention per agent ---
        # reshape to (B*A, T, d_model)
        h_tm = h_sp.permute(0, 2, 1, 3).reshape(B * A, T, -1)
        # add time positional encoding
        h_tm = self.time_pos_encoder(h_tm)
        # to (T, B*A, d_model) for Transformer
        h_tm = h_tm.transpose(0, 1)
        h_tm = self.temporal_encoder(h_tm)
        # back to (B, A, T, d_model)
        h_tm = h_tm.transpose(0, 1).reshape(B, A, T, -1)
        h_tm = self.temporal_norm(h_tm)

        # Final norm + heads
        h_final = self.final_norm(h_tm)
        recon = self.reconstruct(h_final)
        last  = h_final[:, :, -1, :]
        fut   = self.forecast(last).view(B, A, self.future_steps, self.output_dim)

        return fut


class AutoRegressiveMLP(nn.Module):
    def __init__(self,
                 num_features: int,
                 hidden_dim: int = 512,
                 output_dim: int = None,
                 future_steps: int = FUTURE_STEPS):
        """
        :param num_features: number of input features per timestep (F)
        :param hidden_dim:   dimensionality of the MLP’s hidden layers
        :param output_dim:   per‐feature output dim (defaults to num_features)
        :param future_steps: how many steps ahead to forecast (kept simple here)
        """
        super().__init__()
        self.output_dim   = output_dim or num_features
        self.future_steps = future_steps

        # 1) project raw features → hidden_dim
        self.input_proj = nn.Linear(num_features, hidden_dim)

        # 2) a tiny MLP applied per “agent×time token”
        self.mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
        )

        # 3) head to reconstruct each input step (optional)
        self.reconstruct = nn.Linear(hidden_dim, self.output_dim)

        # 4) head to forecast future_steps
        self.forecast = nn.Linear(hidden_dim, future_steps * self.output_dim)

    def forward(self, input_data):
        """
        x: Tensor of shape (B, A, T, F), or a DataBatch with .x & .num_graphs
        returns:
          • recon: (B, A, T, output_dim)
          • fut  : (B, A, future_steps, output_dim)
        """
        # --- unpack DataBatch if needed ---
        if hasattr(input_data, "x") and hasattr(input_data, "num_graphs"):
            # assume every graph has A=50 agents, T=50 timesteps, F=6 features
            B = input_data.num_graphs
            A, T, F = 50, 50, 6
            x = input_data.x.view(B, A, T, F)
        else:
            x = input_data  # already (B, A, T, F)

        # only keep the last 10 steps (as in your example)
        x = x[:, :, -10:, :]       # → (B, A, T′=10, F)
        B, A, T, F = x.shape

        # 1) project & flatten agent×time into tokens
        x = self.input_proj(x)     # → (B, A, T, hidden_dim)
        x = x.view(B, A * T, -1)   # → (B, A*T, hidden_dim)

        # 2) per‐token MLP
        h = self.mlp(x)            # → (B, A*T, hidden_dim)

        # 3) reshape back to (B, A, T, hidden_dim)
        h = h.view(B, A, T, -1)

        # 4) reconstruct
        recon = self.reconstruct(h)   # → (B, A, T, output_dim)

        # 5) forecast from the last time‐step
        last = h[:, :, -1, :]                                     # (B, A, hidden_dim)
        fut  = self.forecast(last)                                # (B, A, future_steps*output_dim)
        fut  = fut.view(B, A, self.future_steps, self.output_dim) # (B, A, future_steps, output_dim)

        return fut.squeeze(2)


# === Example usage ===
if __name__ == "__main__":
    batch_size = 8
    num_agents = 5
    seq_length = 20
    num_features = 6

    model = CrossAgentTransformerPredictor(
        num_features=num_features,
        d_model=32,
        nhead=4,
        num_layers=1,
        dim_feedforward=64,
        dropout=0.1,
    )

    dummy_input = torch.rand(batch_size, num_agents, seq_length, num_features)
    output = model(dummy_input)
    print("Output shape:", output.shape)
    # → Output shape: (8, 5, 20, 6)

