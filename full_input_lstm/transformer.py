"""
NOTE: Starter code generated by gpt-o4
"""
import torch
import torch.nn as nn
import math

from constants import NUM_FEATURES, FUTURE_STEPS, PREV_STEPS

class PositionalEncoding(nn.Module):
    """Sinusoidal positional encoding, as in Vaswani et al."""
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                             -(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # shape (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x: (batch, seq_len, d_model)
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len]  # broadcast over batch


class TwoStageTransformerPredictor(nn.Module):
    """
    Two-stage Transformer: first cross-agent attention per timestep,
    then cross-time attention per agent.
    """
    def __init__(self,
                 num_features: int,
                 d_model: int = 64,  # orig 64
                 nhead: int = 4,
                 num_layers_spatial: int = 1,
                 num_layers_temporal: int = 1,
                 dim_feedforward: int = 128,
                 dropout: float = 0.1,
                 future_steps: int = 60,
                 output_dim: int = None):
        super().__init__()
        self.output_dim   = output_dim or num_features
        self.future_steps = future_steps

        # ─── embedding tables ─────────────────────────────────────────────
        num_agents = 50
        seq_len = 50

        # Input projection + norm
        self.input_proj = nn.Linear(num_features, d_model)
        self.input_norm = nn.LayerNorm(d_model)

        # 1) Spatial encoder: cross-agent attention
        spatial_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation='gelu'
        )
        self.spatial_encoder = nn.TransformerEncoder(
            spatial_layer,
            num_layers_spatial
        )
        self.spatial_norm = nn.LayerNorm(d_model)

        # 2) Positional encoding for time axis
        self.time_pos_encoder = PositionalEncoding(d_model)

        # 3) Temporal encoder: cross-time attention
        temporal_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation='gelu'
        )
        self.temporal_encoder = nn.TransformerEncoder(
            temporal_layer,
            num_layers_temporal
        )
        self.temporal_norm = nn.LayerNorm(d_model)

        # Final norm + output heads
        self.final_norm = nn.LayerNorm(d_model)
        self.reconstruct = nn.Linear(d_model, self.output_dim)
        self.forecast    = nn.Linear(d_model, self.future_steps * self.output_dim)

    def forward(self, input_data):
        # Unpack DataBatch or use raw tensor
        if hasattr(input_data, 'x') and hasattr(input_data, 'num_graphs'):
            B = input_data.num_graphs
            # assuming x shape is (B*A, T, F)
            # reshape to (B, A, T, F)
            n_agents, seq_len, feat = 50, 50, input_data.x.size(-1)
            x = input_data.x.view(B, n_agents, seq_len, feat)
        else:
            x = input_data  # (B, A, T, F)

        # Keep last N timesteps (optional)
        x = x[:, :, -PREV_STEPS:, :]
        B, A, T, F = x.shape

        # Project to model dim
        h = self.input_proj(x)

        h = self.input_norm(h)            # → (B, A, T, d_model)

        # --- 1) Spatial: cross-agent attention per timestep ---
        # reshape to (B*T, A, d_model) then (A, B*T, d_model)
        h_sp = h.view(B * T, A, -1).transpose(0, 1)
        h_sp = self.spatial_encoder(h_sp)
        # back to (B, A, T, d_model)
        h_sp = h_sp.transpose(0, 1).view(B, T, A, -1).permute(0, 2, 1, 3)
        h_sp = self.spatial_norm(h_sp)

        # --- 2) Temporal: cross-time attention per agent ---
        # reshape to (B*A, T, d_model)
        h_tm = h_sp.permute(0, 2, 1, 3).reshape(B * A, T, -1)
        # add time positional encoding
        h_tm = self.time_pos_encoder(h_tm)
        # to (T, B*A, d_model) for Transformer
        h_tm = h_tm.transpose(0, 1)
        h_tm = self.temporal_encoder(h_tm)
        # back to (B, A, T, d_model)
        h_tm = h_tm.transpose(0, 1).reshape(B, A, T, -1)
        h_tm = self.temporal_norm(h_tm)

        # Final norm + heads
        h_final = self.final_norm(h_tm)
        recon = self.reconstruct(h_final)
        last  = h_final[:, :, -1, :]
        fut   = self.forecast(last).view(B, A, self.future_steps, self.output_dim)

        return fut


class AutoRegressiveMLP(nn.Module):
    def __init__(self,
                 num_features: int,
                 hidden_dim: int = 512,
                 output_dim: int = None,
                 future_steps: int = FUTURE_STEPS):
        """
        :param num_features: number of input features per timestep (F)
        :param hidden_dim:   dimensionality of the MLP’s hidden layers
        :param output_dim:   per‐feature output dim (defaults to num_features)
        :param future_steps: how many steps ahead to forecast (kept simple here)
        """
        super().__init__()
        self.output_dim   = output_dim or num_features
        self.future_steps = future_steps

        # 1) project raw features → hidden_dim
        self.input_proj = nn.Linear(num_features, hidden_dim)

        # 2) a tiny MLP applied per “agent×time token”
        self.mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
        )

        # 3) head to reconstruct each input step (optional)
        self.reconstruct = nn.Linear(hidden_dim, self.output_dim)

        # 4) head to forecast future_steps
        self.forecast = nn.Linear(hidden_dim, future_steps * self.output_dim)

    def forward(self, input_data):
        """
        x: Tensor of shape (B, A, T, F), or a DataBatch with .x & .num_graphs
        returns:
          • recon: (B, A, T, output_dim)
          • fut  : (B, A, future_steps, output_dim)
        """
        # --- unpack DataBatch if needed ---
        if hasattr(input_data, "x") and hasattr(input_data, "num_graphs"):
            # assume every graph has A=50 agents, T=50 timesteps, F=6 features
            B = input_data.num_graphs
            A, T, F = 50, 50, NUM_FEATURES
            x = input_data.x.view(B, A, T, F)
        else:
            x = input_data  # already (B, A, T, F)

        # only keep the last 10 steps (as in your example)
        x = x[:, :, -10:, :]       # → (B, A, T′=10, F)
        B, A, T, F = x.shape

        # 1) project & flatten agent×time into tokens
        x = self.input_proj(x)     # → (B, A, T, hidden_dim)
        x = x.view(B, A * T, -1)   # → (B, A*T, hidden_dim)

        # 2) per‐token MLP
        h = self.mlp(x)            # → (B, A*T, hidden_dim)

        # 3) reshape back to (B, A, T, hidden_dim)
        h = h.view(B, A, T, -1)

        # 4) reconstruct
        recon = self.reconstruct(h)   # → (B, A, T, output_dim)

        # 5) forecast from the last time‐step
        last = h[:, :, -1, :]                                     # (B, A, hidden_dim)
        fut  = self.forecast(last)                                # (B, A, future_steps*output_dim)
        fut  = fut.view(B, A, self.future_steps, self.output_dim) # (B, A, future_steps, output_dim)

        return fut.squeeze(2)


class LSTM(nn.Module):
    def __init__(self, input_dim=NUM_FEATURES, hidden_dim=256, num_layers=1, output_dim=60 * 2, dropout=0.0):
        super(LSTM, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True,
                            dropout=dropout)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.test_time = False

    def forward(self, data):
        if hasattr(data, "x"):
            x = data.x
            if self.test_time:
                x = x.reshape(-1, 50, 50, NUM_FEATURES)
            else:
                x = x.reshape(-1, 50, PREV_STEPS, NUM_FEATURES)  # (batch_size, num_agents, seq_len, input_dim)
        else:
            x = data

        x = x[:, 0, -PREV_STEPS:, :] # Only Consider ego agent index 0
        #B, A, T, F = x.shape
        #x = x.view(B*A, T, F)
        #x_joint = x.permute(0, 2, 1, 3).reshape(B, T, A*F)

        lstm_out, _ = self.lstm(x)
        # lstm_out is of shape (batch_size, seq_len, hidden_dim) and we want the last time step output
        out = self.fc(lstm_out[:, -1, :])

        return out.view(-1, 1, 60, 2)

class GRU(nn.Module):
    def __init__(self, input_dim=6, hidden_dim=256, num_layers=1, output_dim=60 * 6, dropout=0.0):
        super(GRU, self).__init__()
        self.lstm = nn.GRU(input_dim, hidden_dim, num_layers=num_layers, batch_first=True,
                            dropout=dropout)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, data):
        if hasattr(data, "x"):
            x = data.x
            x = x.reshape(-1, 50, 50, 6)  # (batch_size, num_agents, seq_len, input_dim)
        else:
            x = data
        x = x[:, 0, :, :] # Only Consider ego agent index 0
        #B, A, T, F = x.shape
        #x = x.view(B*A, T, F)
        #x_joint = x.permute(0, 2, 1, 3).reshape(B, T, A*F)

        lstm_out, _ = self.lstm(x)
        # lstm_out is of shape (batch_size, seq_len, hidden_dim) and we want the last time step output
        out = self.fc(lstm_out[:, -1, :])

        return out.view(-1, 1, 60, 6)

import torch
import torch.nn as nn
import torch.nn.functional as F

class TSTD(nn.Module):
    def __init__(self,
                 num_features: int,
                 d_model: int = 64,
                 nhead: int = 4,
                 num_layers_spatial: int = 1,
                 num_layers_temporal: int = 1,
                 num_layers_decoder: int = 1,
                 dim_feedforward: int = 128,
                 dropout: float = 0.1,
                 future_steps: int = 60,
                 output_dim: int = 6):
        super().__init__()
        self.future_steps = future_steps
        self.output_dim   = output_dim

        # ── encoder ────────────────────────────
        self.input_proj  = nn.Linear(num_features, d_model)
        self.input_norm  = nn.LayerNorm(d_model)

        # spatial (cross-agent) and temporal encoders (as before)…
        spatial_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation='gelu')
        self.spatial_encoder = nn.TransformerEncoder(spatial_layer, num_layers_spatial)
        self.spatial_norm    = nn.LayerNorm(d_model)

        self.time_pos_enc = PositionalEncoding(d_model)

        temporal_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation='gelu')
        self.temporal_encoder = nn.TransformerEncoder(temporal_layer, num_layers_temporal)
        self.temporal_norm    = nn.LayerNorm(d_model)

        # ── decoder ────────────────────────────
        # each layer does: self-attend on past preds + cross-attend to encoder memory
        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation='gelu')
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers_decoder)
        self.dec_pos_enc = PositionalEncoding(d_model)

        # final projection from d_model to 2D displacement
        self.output_proj = nn.Linear(d_model, output_dim)


    def forward(self, input_data, memory=None, teacher_forcing=None):
        """
        x:       (B, A, T, F) historical features
        memory:  optional precomputed encoder output
        teacher_forcing: if not None, a tensor of shape (B, A, future_steps, output_dim)
                         giving the *ground-truth* future displacements to feed into the decoder.
        """
        if hasattr(input_data, "x") and hasattr(input_data, "num_graphs"):
            # assume every graph has A=50 agents, T=50 timesteps, F=6 features
            B = input_data.num_graphs
            A, T, F = 50, 50, 6
            x = input_data.x.view(B, A, T, F)
        else:
            x = input_data  # already (B, A, T, F)

        B, A, T, F = x.shape
        # 1) encode history exactly as before
        h = self.input_norm(self.input_proj(x))                                 # (B, A, T, d)
        # spatial …
        h_sp = h.view(B*T, A, -1).transpose(0,1)                                 # (A, B*T, d)
        h_sp = self.spatial_encoder(h_sp).transpose(0,1).view(B, A, T, -1)       # (B, A, T, d)
        h_sp = self.spatial_norm(h_sp)
        # temporal …
        h_tm = h_sp.permute(0,2,1,3).reshape(B*A, T, -1)                         # (B*A, T, d)
        h_tm = h_tm + self.time_pos_enc(h_tm)                                    # add pos-enc
        h_tm = self.temporal_encoder(h_tm.transpose(0,1)).transpose(0,1)         # (B*A, T, d)
        h_tm = self.temporal_norm(h_tm).view(B, A, T, -1)                        # (B, A, T, d)

        # flatten agents into batch dimension
        memory = h_tm.reshape(B*A, T, -1).transpose(0,1)  if memory is None else memory
        # now memory: (T, B*A, d_model)

        # 2) prepare decoder inputs
        # if teacher_forcing is given, use its *shifted* ground-truth past
        if teacher_forcing is not None:
            # teacher_forcing: (B, A, future_steps, 2)
            # project to d_model
            tf = teacher_forcing.reshape(B*A, self.future_steps, self.output_dim)
            # embed & add pos enc
            tgt = nn.Linear(self.output_dim, memory.size(-1)).to(x.device)(tf)
            tgt = tgt + self.dec_pos_enc(tgt)
            tgt = tgt.transpose(0,1)  # → (future_steps, B*A, d_model)
        else:
            # inference mode: start from zeros
            tgt = torch.zeros(self.future_steps, B*A, memory.size(-1), device=x.device)
            tgt = tgt + self.dec_pos_enc(tgt)

        # 3) build causal mask so each decoder step can only see earlier steps
        mask = nn.Transformer.generate_square_subsequent_mask(self.future_steps).to(x.device)

        # 4) decode
        dec_out = self.decoder(tgt, memory, tgt_mask=mask)   # (future_steps, B*A, d_model)

        # 5) project to 2D displacements & reshape
        preds = self.output_proj(dec_out)                    # (future_steps, B*A, 2)
        preds = preds.transpose(0,1).view(B, A, self.future_steps, -1)

        return preds

