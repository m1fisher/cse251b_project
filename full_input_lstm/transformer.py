"""
NOTE: Starter code generated by gpt-o4
"""
import torch
import torch.nn as nn
import math

from constants import NUM_AGENTS, NUM_FEATURES, FUTURE_STEPS, PREV_STEPS

class PositionalEncoding(nn.Module):
    """Sinusoidal positional encoding, as in Vaswani et al."""
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                             -(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # shape (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x: (batch, seq_len, d_model)
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len]  # broadcast over batch


class TwoStageTransformerPredictor(nn.Module):
    """
    Two-stage Transformer: first cross-agent attention per timestep,
    then cross-time attention per agent.
    """
    def __init__(self,
                 num_features: int,
                 d_model: int = 64,  # orig 64
                 nhead: int = 4,
                 num_layers_spatial: int = 1,
                 num_layers_temporal: int = 1,
                 dim_feedforward: int = 128,
                 dropout: float = 0.1,
                 future_steps: int = 60,
                 output_dim: int = None):
        super().__init__()
        self.output_dim   = output_dim or num_features
        self.future_steps = future_steps

        # ─── embedding tables ─────────────────────────────────────────────
        num_agents = NUM_AGENTS
        seq_len = PREV_STEPS

        # Input projection + norm
        self.input_proj = nn.Linear(num_features, d_model)
        self.input_norm = nn.LayerNorm(d_model)

        # 1) Spatial encoder: cross-agent attention
        spatial_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation='gelu'
        )
        self.spatial_encoder = nn.TransformerEncoder(
            spatial_layer,
            num_layers_spatial
        )
        self.spatial_norm = nn.LayerNorm(d_model)

        # 2) Positional encoding for time axis
        self.time_pos_encoder = PositionalEncoding(d_model)

        # 3) Temporal encoder: cross-time attention
        temporal_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation='gelu'
        )
        self.temporal_encoder = nn.TransformerEncoder(
            temporal_layer,
            num_layers_temporal
        )
        self.temporal_norm = nn.LayerNorm(d_model)

        # Final norm + output heads
        self.final_norm = nn.LayerNorm(d_model)
        self.reconstruct = nn.Linear(d_model, self.output_dim)
        self.forecast    = nn.Linear(d_model, self.future_steps * self.output_dim)

    def forward(self, input_data):
        # Unpack DataBatch or use raw tensor
        if hasattr(input_data, 'x') and hasattr(input_data, 'num_graphs'):
            B = input_data.num_graphs
            # assuming x shape is (B*A, T, F)
            # reshape to (B, A, T, F)
            n_agents, seq_len, feat = NUM_AGENTS, PREV_STEPS, input_data.x.size(-1)
            x = input_data.x.view(B, n_agents, seq_len, feat)
        else:
            x = input_data  # (B, A, T, F)

        # Keep last N timesteps (optional)
        x = x[:, :, -PREV_STEPS:, :]
        B, A, T, F = x.shape

        # Project to model dim
        h = self.input_proj(x)
        h = self.input_norm(h)            # → (B, A, T, d_model)

        # --- 1) Spatial: cross-agent attention per timestep ---
        # reshape to (B*T, A, d_model) then (A, B*T, d_model)
        h_sp = h.view(B * T, A, -1).transpose(0, 1)
        h_sp = self.spatial_encoder(h_sp)
        # back to (B, A, T, d_model)
        h_sp = h_sp.transpose(0, 1).view(B, T, A, -1).permute(0, 2, 1, 3)
        h_sp = self.spatial_norm(h_sp)

        # --- 2) Temporal: cross-time attention per agent ---
        # reshape to (B*A, T, d_model)
        h_tm = h_sp.permute(0, 2, 1, 3).reshape(B * A, T, -1)
        # add time positional encoding
        h_tm = self.time_pos_encoder(h_tm)
        # to (T, B*A, d_model) for Transformer
        h_tm = h_tm.transpose(0, 1)
        h_tm = self.temporal_encoder(h_tm)
        # back to (B, A, T, d_model)
        h_tm = h_tm.transpose(0, 1).reshape(B, A, T, -1)
        h_tm = self.temporal_norm(h_tm)

        # Final norm + heads
        h_final = self.final_norm(h_tm)
        recon = self.reconstruct(h_final)
        last  = h_final[:, :, -1, :]
        fut   = self.forecast(last).view(B, A, self.future_steps, self.output_dim)

        return fut


class LSTM(nn.Module):
    def __init__(self, input_dim=NUM_FEATURES, hidden_dim=256, num_layers=1, output_dim=60 * 2, dropout=0.0):
        super(LSTM, self).__init__()
        self.lstm = nn.LSTM(NUM_FEATURES * NUM_AGENTS, hidden_dim, num_layers=num_layers, batch_first=True,
                            dropout=dropout)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.test_time = False

    def forward(self, data):
        if hasattr(data, "x"):
            x = data.x
            if self.test_time:
                x = x.reshape(-1, NUM_AGENTS, 50, NUM_FEATURES)
            else:
                x = x.reshape(-1, NUM_AGENTS, PREV_STEPS, NUM_FEATURES)  # (batch_size, num_agents, seq_len, input_dim)
        else:
            x = data
        x = x[:, 0, -PREV_STEPS:, :]

        # gpt-generated reshaping code
        # 3) permute and flatten “agent” axis into the feature dimension:
        #B, A, T, F = x.shape                      # A == self.num_agents, F == self.feat_dim
        #    (B, T, A, F)
        #x = x.permute(0, 2, 1, 3)
        #    (B, T, A * F)
        #x = x.reshape(B, T, A * F)

        lstm_out, _ = self.lstm(x)
        # lstm_out is of shape (batch_size, seq_len, hidden_dim) and we want the last time step output
        out = self.fc(lstm_out[:, -1, :])

        return out.view(-1, 1, 60, 2)

import torch
from torch import nn

class SimpleEncoder(nn.Module):
    def __init__(self, num_agents: int, feat_dim: int, emb_dim: int):
        """
        Encoder that maps each time‐step’s (A*F) input → a lower‐dimensional embedding of size emb_dim.

        Args:
            num_agents (int):  A (number of agents)
            feat_dim   (int):  F (number of features per agent)
            emb_dim    (int):  D (desired embedding size per time‐step)
        """
        super().__init__()
        self.input_dim = num_agents * feat_dim   # A * F
        self.emb_dim   = emb_dim

        # A two‐layer MLP (you can remove the second layer if you want even simpler)
        self.fc1 = nn.Linear(self.input_dim, emb_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(emb_dim, emb_dim)

    def forward(self, x_flat: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x_flat: shape (B, T, A*F)

        Returns:
            emb:   shape (B, T, D)
        """
        B, T, _ = x_flat.shape              # _ == A*F
        # Flatten batch & time to run through the same fc layers:
        x2d = x_flat.view(B * T, self.input_dim)   # → (B*T, A*F)

        h = self.relu(self.fc1(x2d))        # (B*T, D)
        h = self.relu(self.fc2(h))          # (B*T, D)

        return h.view(B, T, self.emb_dim)   # → (B, T, D)

